---
title: "Bayesian regression"
author: "Tristan Mahr"
date: "April 13, 2017"
output:
  github_document: default
  beamer_presentation:
    includes:
      in_header: header.txt
    latex_engine: pdflatex
    theme: Metropolis
    toc: no
  ioslides_presentation: default
  slidy_presentation: default
subtitle: subtitle
---

```{r setup, include = FALSE}
options(width = 60)
knitr::opts_knit$set(width = 60)

knitr::opts_chunk$set(
  echo = FALSE,
  comment = "#>",
  collapse = TRUE)

library(dplyr)
library(ggplot2)
library(extrafont)
library(hrbrthemes)

extrafont::loadfonts()

my_theme <- theme_ipsum(base_size = 24, axis_title_size = 20)

theme_set(my_theme)
```

## Overview

* A little about me and how I got into Bayes
* Mathematical intuition building
* Bayesian updating
* Fitting a model with RStanARM

# Background

## About me 

* I am dissertator in Communication Sciences and Disorders
* I study word recognition in preschoolers
* For statistics, I do multilevel logistic regression models
* R enthusiast

## I was once in your shoes

I learned stats and R in this course with Markus Brauer and John Curtin.

I _still_ refer to the slides from this course on contrast codes.

But now I'm a "Bayesian".


## August 2015: The "Crisis" in Psychology

[Open Science Collaboration (2015)][osf2015] tries to replicate 100 studies
published in 3 psychology different journals in 2008.

- Boil a study down to 1 test statistic and 1 effect size.
- Replicate the study.
- Compare replication's test statistical and effect size against original.

----

![Scatter plot of original vs replicated effect sizes](./assets/reproducibility.PNG)

----

- Approximately 36% of the studies are replicated (same test statistic.
- On average, effect sizes in replications are half that of the original
  studies.



## Reactions

![I don't know how to turn off the figure labeling feature](./assets/this-is-fine.jpeg)


## Reactions

> * We're doomed.
> * Most findings are probably false, and we knew that already.
> * No, this is business as usual.
> * Any credible discipline has to do this kind of house-cleaning from time to time.

## Lots of hand wringing and soul searching

Some reactionary:

- Replication creates an [industry for incompetent hacks][replication-hacks].
- Here come the [methodological terrorists][method-terrorists]!

Some constructive:

- [Everything is f'ed][everything-fucked] -- so what else is new?
- Increased rigor and openness are [a good thing][repeat-after-me].

[replication-hacks]: http://www.sciencedirect.com/science/article/pii/S002210311600007X
[method-terrorists]: http://andrewgelman.com/2016/09/21/what-has-happened-down-here-is-the-winds-have-changed/
[everything-fucked]: https://hardsci.wordpress.com/2016/08/11/everything-is-fucked-the-syllabus/
[repeat-after-me]: https://thenib.com/repeat-after-me


## Crisis made me think more about questionable practices

All those unintentional acts and rituals to appease the Statistical Significance
gods.

. . . 

HARKing 
~ Hypothesizing after results are known.
~ Telling a story to fit the data.

. . . 

Garden of forking data
~ Conducting countless sub-tests and sub-analyses on the data.

. . . 

_p_-hacking
~ Doing these tests in order to find a significant effect.

. . . 

Selective reporting
~ Reporting only the tests that yielded a significant result.

----

The usual way of doing things is _insecure_.

. . . 

> - Perfectly fine if you know what you're doing.
> - Works great if you pre-register analyses. Provides error control.
> - But vulnerable to exploitation.

----

## My response to the crisis

I want to avoid these questionable practices.

I want to level up my stats and explore new techniques.

- Maybe more robust estimation techniques?
- Maybe machine learning techniques to complement conventional analyses?

I want something less finicky than statistical significance.

- _p_-values don't mean what many people think they mean.
- Neither do confidence intervals.
- Statistical significance is not related to practical significance.



## December 2015

![Cover of Data Analysis USing Regression and Multilevel/Hierarchical Models](./assets/arm.jpeg)

---

I started reading the Gelman and Hill book.

- This is the book for [the `arm` package][arm].
- Still the best treatment of multilevel models in R despite being 10 years old.

It emphasizes estimation, uncertainty and simulation.

. . . 

Midway through, the book pivots to Bayesian estimation. (Multilevel models are
kind of Bayesian because they borrow information across different clusters.)


## January 2016

I'm down a rabbit hole, writing Stan (Bayesian) models to fit the models from the ARM
book, and there is an influx of Bayesian tools for R.

- [Statistical Rethinking][rethinking], a book that reteaches regression from a
  Bayesian perspective with R and Stan, is released.
- New version of [brms][brms] is released. This package converts R model code
  into Stan programs.
- [RStanARM][rstanarm] is released.
- A blog post circulates: ["R Users Will Now Inevitably Become Bayesians"]
  [we-r-bayesians].

I eat all this up. I become a convert.

[rethinking]: http://xcelab.net/rm/statistical-rethinking/
[brms]: https://cran.r-project.org/web/packages/brms/index.html
[rstanarm]: https://cran.r-project.org/web/packages/rstanarm/index.html
[we-r-bayesians]: https://thinkinator.com/2016/01/12/r-users-will-now-inevitably-become-bayesians/



## Long story short

The replication crisis sparked my curiosity, and a wave of new tools and
resources made it really easy to get started with Bayesian stats.

My goal with this approach has been to make better, more honest scientific
summaries of observed data.

# Classical versus Bayesian in two plots

## The data

```{r, echo=TRUE}
## Some toy data
davis <- car::Davis %>% filter(100 < height) %>% as_data_frame()
davis
```

----

## The data

```{r height-weight-sex}
ggplot(davis) + 
  aes(x = height, y = weight) + 
  geom_point() + 
  labs(x = "height (cm)", y = "weight (kg)")
```

## Classical model shows line of best fit

```{r hws-lm}
ggplot(davis) + 
  aes(x = height, y = weight) + 
  geom_point() + 
  stat_smooth(method = "lm", se = FALSE) + 
  labs(x = "height (cm)", y = "weight (kg)")
```

## Bayesian model's median line of fit


```{r hws-stan}
n_draws <- 100
alpha_level <- .15
col_draw <- "grey60"
col_median <-  "#3366FF"

m <- rstanarm::stan_glm(weight ~ scale(height), davis, family = gaussian())

davis2 <- davis %>% mutate(Observation = seq_along(height))

draws100 <- rstanarm::posterior_linpred(m) %>% 
  as_data_frame() %>% 
  sample_n(100) %>% 
  as.matrix() %>% 
  reshape2::melt(c("Draw", "Observation")) %>% 
  left_join(davis2)

draws20 <- rstanarm::posterior_linpred(m) %>% 
  as_data_frame() %>% 
  sample_n(20) %>% 
  as.matrix() %>% 
  reshape2::melt(c("Draw", "Observation")) %>% 
  left_join(davis2)


draws100 <- rstanarm::posterior_linpred(m) %>% 
  as_data_frame() %>% 
  sample_n(n_draws) %>% 
  as.matrix() %>% 
  reshape2::melt(c("Draw", "Observation")) %>% 
  left_join(davis2)

drawsmedian <- rstanarm::posterior_linpred(m) %>% 
  as.matrix() %>% 
  reshape2::melt(c("Draw", "Observation")) %>% 
  group_by(Observation) %>% 
  summarise(Estimate = median(value)) %>% 
  left_join(davis2)


ggplot(davis) + 
  aes(x = height, y = weight) + 
  geom_point() + 
  geom_line(aes(y = Estimate), data = drawsmedian,size = 1, color = col_median) +
  labs(x = "height (cm)", y = "weight (kg)")
```


## Median line and 20 other lines from posterior

```{r}
ggplot(davis) + 
  aes(x = height, y = weight) + 
  geom_point() + 
  geom_line(aes(y = value, group = Draw), data = draws20, color = col_draw, alpha = .8) +
  geom_line(aes(y = Estimate), data = drawsmedian,size = 1, color = col_median) +
  labs(x = "height (cm)", y = "weight (kg)")
```

## Median line and 100 other lines from posterior

```{r}
ggplot(davis) + 
  aes(x = height, y = weight) + 
  geom_point() + 
  geom_line(aes(y = value, group = Draw), data = draws100, color = col_draw, alpha = .3) +
  geom_line(aes(y = Estimate), data = drawsmedian,size = 1, color = col_median) +
  labs(x = "height (cm)", y = "weight (kg)")
```


----

# Bayesian sale pitch

## Benefits

The models provide intuitive results.

- When we misinterpret _p_-values or confidence intervals, we usually are
  interpreting them in a Bayesian way.
- Bayesian uncertainty intervals are what we want from confidence intervals.

----

Bayesian models quantify uncertainty.

- Basically, if a classical model can estimate or predict something about the 
  data, the Bayesian model can estimate a distribution for that thing too.
- Bayesian models are generative, and the posterior predictive distribution 
  (which simulates fake data using the model) is a useful tool.


----

Bayesian models incorporate prior information.

- That information can be weak, moderate or strong.
- I don't say "prior beliefs" because that sounds too subjective.
- All models make assumptions and prior information, and priors make that 
  information explicit.
 
----

Bayesian models are flexible.

- This course captures a bag of tricks (_t_-tests, ANOVA, ANCOVA, mixed 
  effects) under a general framework (it's all regression).
- Bayesian regression incorporates even more tricks (missing data imputation, 
  measurement error models, robust error models) into the framework.

----

Bayesian models have computational benefits.

- Multilevel models with lots of random effects probably won't converge.
- But some weak prior information will nudge the models in the right direction 
  and make the models work.

## Downsides

It's different.

- People are really used to signficance testing and _p_-values, so you have to 
  do more hand-holding when explaining results.
- You don't get to say _significant_ anymore. (I use _plausible_ and 
  _credible_.)
- People have misconceptions about subjectivism and bias.


----

More work before and after modeling.

- You need to specify priors for your parameters. 
- Your model is a distribution, so you have to do a bit more work wrangling 
  the data.

----

It takes longer.

- Classical models solve an optimization problem and provide a single set of 
  parameter estimates. 
- MCMC sampling explores the space of parameter values and provides thousands of 
  parameter estimates.
- It can take a few hours to fit a complicated multilevel model.

---- 

It's not a cure-all. There are still insecurities.

- It's statistics and people can still misunderstand the methods and models. 
- A motivated _p_-hacker can still exploit Bayes factors, which is why I won't
  discuss them.





# Building mathematical intuitions


## Caveat

* These slides and these examples are meant to illustrate the pieces of Bayes
  theorem.
* This is not a rigorous mathematical description of Bayesian probability
  or regression.


## Conditional probability review

$$ p(A \mid B) : \text{probability of A given B}$$

. . .

Suppose that 95% of emails with the phrase "investment opportunity" are spam.

. . .

$$ p(\text{spam email} \mid \text{"investment opportunity"})  = .95 $$


## Conditional probability review

What would this probability express?

$$ p(\text{"investment opportunity"} \mid \text{spam email})$$

. . .

That ordering matters. $p(A \mid B)$ is not the same as $p(B \mid A)$.




## Bayes theorem

A theorem about conditional probability.

$$ p(B \mid A) = \frac{ p(A \mid B) * p(B)}{p(A)} $$

----

I can never remember this equation with letters. Here's how I prefer to write it.

$$ p(\text{hypothesis} \mid \text{data}) = \frac{ p(\text{data} \mid \text{hypothesis}) * p(\text{hypothesis})}{p(\text{data})} $$

---

$$ p(\text{hypothesis} \mid \text{data}) = \frac{ p(\text{data} \mid \text{hypothesis}) * p(\text{hypothesis})}{p(\text{data})} $$

The "hypothesis" is typically something unobserved or unknown. It's what you 
want to learn about using the data. 

. . . 

For regression models, the "hypothesis" is a parameter (intercept, slopes or 
error terms).

Bayes theorem tells you the probability of the hypothesis given the data.



## General structure

How plausible is some hypothesis given the data?

$$ p(\text{hypothesis} \mid \text{data}) = \frac{ p(\text{data} \mid \text{hypothesis}) * p(\text{hypothesis})}{p(\text{data})} $$

. . .

Pieces of the equation:

$$ \text{posterior} = \frac{ \text{likelihood} * \text{prior}}{\text{average likelihood}} $$


## Classifying emails

I got an email with the word "cialis" in it. Is it spam?

. . .

- What I want to know is spam-ness (versus ham-ness). 
- What I have is an email with the word "cialis".

. . .

$$ 
P(\text{spam} \mid \text{"cialis"}) = 
  \frac{P(\text{"cialis"} \mid \text{spam}) * P(\text{spam})}
       {P(\text{"cialis"})}
$$


## Email example

The two unconditional probabilities are base rates that need to be accounted
for.

. . .

The prior is the frequency of spam in general. The average likelihood is the
frequency of the word "cialis" in emails.

$$ 
P(\text{spam} \mid \text{"cialis"}) = 
  \frac{\text{"cialis" freq. in spam} * \text{spam rate}}
  {\text{"cialis" freq.}} 
$$


## "Bayesianism" 

Some people would argue that using Bayes theorem is not "Bayesian". After
all, in this example, we're just counting the frequency of events. 

It's kind of weird, but it is also true. 

Simple event-counting is not what people usually mean by the word "Bayesian". 

--------


## The "Bayesianism" form of Bayes' theorem

$$ 
\text{updated information} = 
  \frac{\text{likelihood of data} * \text{prior information}}
       {\text{average likelihood of data}} 
$$

Bayes' theorem provides a systematic way to update our knowledge as we encounter new data.

. . .

$$ 
\text{updated beliefs} \propto 
  \text{likelihood of data} * \text{prior beliefs} 
$$

- Update your beliefs in proportion to how well the data fits those beliefs.
- Your beliefs have probabilities. You can quantify your uncertainty about what you know.



# Okay, but what is likelihood?


----

_Sidenote: This is nifty. A lot of my stats training made more sense
once I had a broader understanding of likelihood._

----

## First, what are models?!

What is a statistical model?

. . .

It's a description of how the data could have been generated.


## IQ example

> IQ scores are normally distributed.

. . .

$$
\mathrm{IQ}_i \sim \mathrm{Normal}(\underbrace{\mu}_{\text{mean}}, \underbrace{\sigma}_{\text{SD}})
$$

$\mu$ and $\sigma$ are parameters for this model that change the center and
spread of the normal bell curve.

The normative IQ model has $\mu = 100$  and $\sigma = 15$.



## Likelihood measures fit

How likely are the data in a given model? 

. . . 

I never see it explained this way, but I think of likelihood as "fit".

How the well data fits in a given model.


## An IQ example

We found some IQ scores in an old, questionable dataset.

```{r some-code, echo = TRUE}
library(dplyr)
iqs <- car::Burt$IQbio
iqs
```

. . . 

IQs are designed to have a normal distribution with a population mean of 100
and an SD of 15.

How well do these data *fit* in that kind of bell curve?


## Density as height on a bell curve

```{r, echo = FALSE}
iq_df <- function(mean, sd = 15, xs = iqs) {
  data_frame(
    iq = seq(min(xs), max(xs), length.out = 100),
    density = dnorm(iq, mean, 15),
    mean = mean,
    sd = sd)
}
```

```{r iq-density-bell-curve, echo = FALSE, fig.cap = "A hypothetical bell curve with a mean of 100 and SD of 15."}
p <- ggplot(iq_df(100, 15)) +
  aes(iq, density) +
  geom_line(size = 1) + 
  ylim(c(0, NA))
p
```

## Density measures likelihood

```{r iq-density-bell-curve-2, echo = FALSE, fig.cap = "Likelihood of an IQ of 90"}
one_iq <- data_frame(iq = 90, density = dnorm(90, 100, 15), yend = 0, xend = 90)

p + 
  geom_segment(aes(xend = xend, yend = yend), data = one_iq, size = 1) +
  geom_point(data = one_iq, size = 3)
```



---

* Height of each point on curve is density around that point.
* Higher density regions are more likely.
* Data farther from peak density is less likely.

----

```{r iq-density-bell-curve-3, echo = FALSE, fig.cap = "Density of IQ scores drawn a bell curve with mean 100."}
p <- ggplot(iq_df(100, 15)) +
  aes(iq, density) +
  geom_line(size = 1) +
  geom_point(aes(x = iqs, y = dnorm(iqs, 100, 15)), data = data_frame(iqs), size = 3) +
  geom_segment(aes(x = iqs, xend = iqs, y = 0, yend = dnorm(iqs, 100, 15)),
               data = data_frame(iqs), size = 1) 
p
```



----

```{r iq-density-bell-curve-4, echo = FALSE, fig.cap = "Density of IQ scores drawn a bell curve with mean 130. The fit is terrible."}
bad_mean <- 130

p <- ggplot(iq_df(bad_mean, 15)) +
  aes(iq, density) +
  geom_line(size = 1) +
  geom_point(aes(x = iqs, y = dnorm(iqs, bad_mean, 15)), data = data_frame(iqs), size = 3) +
  geom_segment(aes(x = iqs, xend = iqs, y = 0, yend = dnorm(iqs, bad_mean, 15)),
               data = data_frame(iqs), size = 1) 
p
```


----

Density function `dnorm(xs, mean = 100, sd = 15)` tells us the height of each
value in `xs` when drawn on a normal bell curve.

```{r}
# likelihood (density) of each point
dnorm(iqs, 100, 15) %>% round(3)
```

. . . 

Likelihood of all points is the product. These quantities get vanishingly small
[so we sum their logs instead][math-ex-ll]. (Hence, **log-likelihoods**.)

```{r}
# vanishingly small!
prod(dnorm(iqs, 100, 15))

# log scale
sum(dnorm(iqs, 100, 15, log = TRUE))
```

[math-ex-ll]: https://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution

----

Log-likelihoods provide a measure of how well the data fit a given normal
distribution.

Which mean best fits the data? Below average IQ (85), average IQ (100), or above
average IQ (115)? (Higher is better.)

. . . 

```{r}
sum(dnorm(iqs, 85, 15, log = TRUE))
sum(dnorm(iqs, 100, 15, log = TRUE))
sum(dnorm(iqs, 115, 15, log = TRUE))
```

. . .

Of these three, the data fit best with the "population average" mean (100).

We just used a **maximum likelihood** criterion to choose among these
alternatives!




## Likelihood summary

We have some model of how the data could be generated. This model has tuneable 
parameters.

> The IQs are drawn from a normal distribution with an SD of 15 and some 
unknown mean.

Likelihood is how well the observed data fit in a particular data-generating 
model.

Classical regression's "line of best fit" finds model parameters that maximize 
the likelihood of the data.



# Bayesian models

$$ \text{posterior} = \frac{ \text{likelihood} * \text{prior}}{\text{average likelihood}} $$

A Bayesian model examines a distribution over model parameters. What are all the
plausible ways the data could have been generated?

- Prior: A probability distribution over model parameters.
- Update our prior information in proportion to how well the data fits with
  that information.




## Bayesian updating


Let's consider all integer values from 70 to 130 as equally probable means for
the IQs. This is a flat or uniform prior.

Here's our model.

$$
\begin{align*}
   \mathrm{IQ}_i &\sim \text{Normal}(\mu, \sigma = 15) &\text{[likelihood]}
   \\
  \mu &\sim \{\text{integers from 70 to 130}\} &\text{[prior for }\mu]
\end{align*}
$$

----

We are going to use **grid approximation** for this example. That means 
systematically exploring about a bunch of parameter values.

```{r, echo = TRUE}
# Keep track of last probability value
df_iq_model <- data_frame(
  mean = 70:130,
  prob = 1 / length(mean),
  previous = NA_real_)
```

----

```{r, echo = TRUE}
sum(df_iq_model$prob)
df_iq_model
```

----

```{r iq-00-data, echo = FALSE}
ggplot(df_iq_model) +
  aes(x = mean, y = prob) +
  geom_line(size = 1) +
  ylim(c(0, .10)) +
  xlab("possible mean") +
  ylab("probability") +
  ggtitle("Data observed: 0")
```




----

We observe one data-point, $y = `r iqs[1]`$, and update our prior information
using the likelihood of the data at each possible mean.

```{r, echo = TRUE}
df_iq_model$previous <- df_iq_model$prob
likelihoods <- dnorm(iqs[1], df_iq_model$mean, 15)
# numerator of bayes theorem
df_iq_model$prob <- likelihoods * df_iq_model$prob
sum(df_iq_model$prob)
```

. . . 

That's not right! We need the *average likelihood* to ensure that the
probabilities add up to 1. This is why it's sometimes called a *normalizing
constant*.

```{r, echo = TRUE}
# include denominator of bayes theorem
df_iq_model$prob  <- df_iq_model$prob / sum(df_iq_model$prob)
sum(df_iq_model$prob)
```




----

```{r iq-01-data, echo = FALSE, fig.cap = NULL}
ggplot(df_iq_model) +
  aes(x = mean, y = prob) +
  geom_line(size = 1) +
  geom_line(aes(y = previous), linetype = "dashed", size = 1) +
  geom_rug(aes(x = iqs, y = iqs), data_frame(iqs = iqs[1]), sides = "b",
           size = 1.2) +
  ylim(c(0, .10)) +
  xlab("possible mean") +
  ylab("probability") +
  ggtitle("Data observed: 1")
```




----

We observe another data-point and update the probability with the likelihood
again.

```{r, echo = TRUE}
df_iq_model$previous <- df_iq_model$prob
likelihoods <- dnorm(iqs[2], df_iq_model$mean, 15)
df_iq_model$prob <- likelihoods * df_iq_model$prob
# normalize
df_iq_model$prob <- df_iq_model$prob / sum(df_iq_model$prob)
df_iq_model
```




----

```{r iq-02-data, echo = FALSE, fig.cap = NULL}
ggplot(df_iq_model) +
  aes(x = mean, y = prob) +
  geom_line(size = 1) +
  geom_line(aes(y = previous), linetype = "dashed", size = 1) +
  geom_rug(aes(x = iqs, y = iqs), data_frame(iqs = iqs[1:2]), sides = "b") +
  geom_rug(aes(x = iqs, y = iqs), data_frame(iqs = iqs[2]), sides = "b",
           size = 1.2) +
  ylim(c(0, .10)) +
  xlab("possible mean") +
  ylab("probability") +
  ggtitle("Data observed: 2")
```




----

And one more...

```{r, echo = TRUE}
df_iq_model$previous <- df_iq_model$prob
likelihoods <- dnorm(iqs[3], df_iq_model$mean, 15)
df_iq_model$prob <- likelihoods * df_iq_model$prob
# normalize
df_iq_model$prob <- df_iq_model$prob / sum(df_iq_model$prob)
df_iq_model
```




----

```{r iq-03-data, echo = FALSE, fig.cap = NULL}
ggplot(df_iq_model) +
  aes(x = mean, y = prob) +
  geom_line(size = 1) +
  geom_line(aes(y = previous), linetype = "dashed", size = 1) +
  geom_rug(aes(x = iqs, y = iqs), data_frame(iqs = iqs[1:3]), sides = "b") +
  geom_rug(aes(x = iqs, y = iqs), data_frame(iqs = iqs[3]), sides = "b",
           size = 1.2) +
  ylim(c(0, .10)) +
  xlab("possible mean") +
  ylab("probability") +
  ggtitle("Data observed: 3")
```

----

https://github.com/tjmahr/MadR_RStanARM/blob/master/assets/simple-updating.gif

----




# Connecting Bayes' theorem to linear regression


## Linear models

I learned stats in this course, so you probably write regression models as a
one-liner like:


$$ \underbrace{y_i}_{\text{observation}} = 
   \underbrace{\alpha + \beta _{1} x_{1i}}_{\text{predicted mean given }x} + 
   \underbrace{\epsilon _i}_{\text{random error}} $$ 

. . . 

Data generating model: Observation $y_i$ is a draw from a normal distribution
centered around a mean.

We estimate the mean with a constant "intercept" term $\alpha$ plus a linear
combination of predictor variables (just $x_1$ for now).

----

Let's re-write the model to make the normal-distribution part clear. No more 
one-liner. (The $\sim$ means "sampled from".)

$$
\begin{align*}
   y_i &\sim \text{Normal}(\text{mean} = \mu_i, \text{SD} = \sigma) &\text{[likelihood]}
   \\
  \mu_i &= \alpha + \beta_1*x_{1i} &\text{[linear model]}
\end{align*}
$$

. . . 

Observation $y_i$ is a draw from a normal distribution
centered around a mean $\mu_i$ with a standard deviation of $\sigma$.

The mean is a constant term $\alpha$ plus a linear combination of predictor
variables (just $x_1$ for now).

----

(These equations describe the same models. It's just a different kind of
notation.)

----


## Weight by height model

Consider a model of weight predicted by height... 

---

```{r staggered-bell-curves, echo = FALSE, fig.cap = "It's like a tunnel of bell curves. The center of it moves with _x_.", message = FALSE, warning = FALSE}
# options(hrbrthemes.loadfonts = TRUE)

# Some toy data
davis <- car::Davis %>%
  filter(100 < height)

# Fit a model and estimate mean at five points
m <- lm(weight ~ height, davis)
newdata <- data_frame(height = c(15:19 * 10))
newdata$fit <- predict(m, newdata)

# get density of random normal values
get_density_df <- function(mean, sd, steps) {
  ends <- qnorm(c(.001, .999), mean, sd)
  steps <- seq(ends[1], ends[2], length.out = steps)

  df <- data_frame(
    value = steps,
    density = dnorm(steps, mean, sd))
  df
}

# Get a distribution at each mean
simulated <- newdata %>%
  group_by(height) %>%
  do(get_density_df(.$fit, sigma(m), 10000)) %>%
  ungroup


ggplot(simulated) +
  # Plot at each mean, adding some scaled value of density to the mean.
  aes(x = height - (100 * density), y = value, group = height) +
  geom_polygon(fill = "grey50") +
  # raw data
  geom_point(aes(height, weight), data = davis) +
  labs(x = "height (cm)", y = "weight (kg)") 
```


## Bayesian stats

To make the model Bayesian, we need to give prior distributions to parameters.

The parameters we need to estimate for regression: $\alpha, \beta_1, \sigma$. 

. . .

$$
\begin{align*}
   y_i &\sim \text{Normal}(\mu_i, \sigma) &\text{[likelihood]}
   \\
  \mu_i &= \alpha + \beta_1*x_{1i} &\text{[linear model]} \\
  \alpha &\sim \text{Normal}(0, 10) &\text{[prior for }\alpha] \\ 
  \beta_1 &\sim \text{Normal}(0, 5) &\text{[prior for }\beta_1] \\ 
  \sigma &\sim \text{HalfCauchy}(0, 5) &\text{[prior for }\sigma] \\ 
\end{align*}
$$

## What's the point?

- A classical model provides one model of many plausible models of the data.
  It'll find the parameters that maximize likelihood. 
- A Bayesian model is a model of models. We get a *distribution of models* that
  are consistent with the data.




## But this is where things get difficult


Parameters we need to estimate: $\alpha, \beta_1, \sigma$


$$ \mathrm{posterior} = \frac{ \mathrm{likelihood} * \mathrm{prior}}{\mathrm{average\ likelihood}} $$
. . . 

$$ P(\alpha, \beta, \sigma \mid x) = \frac{ P(x \mid \alpha, \beta, \sigma) \, P(\alpha, \beta, \sigma)}{\iiint \, P(x \mid \alpha, \beta, \sigma) \, P(\alpha, \beta, \sigma) \,d\alpha \,d\beta \,d\sigma} $$

Things get gnarly. This is the black-box step.

## Good news

We don't perform this integral calculus. 

Insead, we rely on Markov-chain Monte Carlo simulation to get samples from the 
posterior. 

Those samples will provide a detailed picture of the posterior.


# Finally, let's fit a model



## An example: Height and Weight by Sex

```{r}
# Some toy data
davis <- car::Davis %>% filter(100 < height) %>% as_data_frame()
davis
```

--- 

## Classical linear model

```{r}
mean(davis$height)
davis$heightC <- davis$height - mean(davis$height)

m <- glm(weight ~ heightC * sex, davis, family = gaussian())
m %>% summary() %>% coef() %>% round(3)
```


## What is RStanARM?

Stan: a probabalistic programming language / MCMC sampler

RStanARM: RStan Applied Regression Modeling

- Batteries-included versions of common regression models.
- `glm` -> `stan_glm`, `glmer` -> `stan_glmer`.
- [CRAN page](https://cran.r-project.org/web/packages/rstanarm/)
  - Very good! They have lots of detailed vignettes!
- Proper successor to the `arm` package.


---

```{r, echo = TRUE}
library(rstanarm)
```

* So... hard-code the priors.
* I only set the `mc.cores` option when I have a big model that going to take
  more than a minute to fit.




## Fit the model

We have to use `stan_glm()`.

* `stan_lm()` uses a different specification of the prior.

By default, it does sampling with 4 MCMC chains.

* Each chain is 2000 samples, but the first half are warm-up samples.
* Warm-up samples are ignored

----

```{r stan-davis, echo = TRUE, cache = FALSE}
stan_model <- stan_glm(
  weight ~ heightC * sex,
  data = davis,
  family = gaussian,
  prior = normal(0, 5),
  prior_intercept = normal(0, 10)
)
```

## Printing the model

```{r, echo = TRUE}
stan_model
```

## One note

Predictors are centered and rescaled internally by rstanarm, so our priors are 
on the standardized scale.

* `normal(0, 5)` is a distribution of effect sizes with mean 0 and SD 5

See `?rstanarm::priors`, esp. the `autoscale` argument.

```{r, echo = TRUE}
prior_summary(stan_model)
```


## Getting a summary from the model

```{r}
summary(stan_model)
```




## Notes on summary()

* Split into estimation and diagnostic information
* `mean_PPD` is the predicted value for a completely average observation


## The cut to the chase plot

Here is what classical linear regression does.

```{r}
ggplot(davis) + 
  aes(x = heightC, y = weight, color = sex) + 
  geom_point(alpha = .5) + 
  stat_smooth(method = "lm", se = FALSE)
```


```{r}

part1 <- davis %>% 
  filter(sex == "M") %>% 
  modelr::data_grid(heightC) %>% 
  mutate(sex = "M")

part2 <- davis %>% 
  filter(sex == "F") %>% 
  modelr::data_grid(heightC) %>% 
  mutate(sex = "F")

grid <- bind_rows(part1, part2)
grid$Point <- seq_len(nrow(grid))

done <- posterior_linpred(stan_model, newdata = grid, draws = 200) 

long <- done %>%
  as_data_frame() %>% 
  sample_n(100) %>% 
  as.matrix() %>% 
  reshape2::melt(varnames = c("Draw", "Point")) %>% 
  left_join(grid)


# df_model <- stan_model %>% as_data_frame()
# df_model
# 
# df_model2 <- df_model %>%
#   mutate(F_Intercept = `(Intercept)`, F_Slope = heightC,
#          M_Intercept = `(Intercept)` + sexM,
#          M_Slope = heightC + `heightC:sexM`) %>%
#   select(F_Intercept:M_Slope)
# df_model2

# 
# fits <- sample_n(df_model2, 200)
# medians <- df_model2 %>% summarise_each(funs = funs(median))

p2 <- ggplot(long) +
  aes(x = heightC, y = value, color = sex) + 
  geom_line(aes(group = interaction(Draw, sex)), alpha = .1)
p2
  


geom_abline(aes(color = "F", intercept = F_Intercept,
                  slope = F_Slope), data = fits, alpha = .075) +
  geom_abline(aes(color = "M", intercept = M_Intercept,
                  slope = M_Slope), data = fits, alpha = .075) +
  geom_abline(aes(color = "F", intercept = F_Intercept,
                  slope = F_Slope), data = medians, size = 1.25) +
  geom_abline(aes(color = "M", intercept = M_Intercept,
                  slope = M_Slope), data = medians, size = 1.25) +
  geom_point() +
  theme(legend.position = c(0, 1), legend.justification = c(0, 1))
```

```


# Inspecting posterior samples




## Looking at the posterior parameter samples

Coerce to a data-frame. Columns are parameters. One row per posterior sample.

```{r, echo = TRUE}
samples <- stan_model %>% as.data.frame() %>% tbl_df()
samples
```




## Looking at the posterior parameter samples

```{r histogram-of-height-effect, fig.cap = "Histogram of height effect."}
ggplot(samples) + 
  aes(x = heightC) + 
  geom_histogram() + 
  xlab("Height effect") + 
  ylab("N posterior samples")
```




## Quantiles are post-data probabilities

If we believe there is a "true" value for a parameter, there is 90% 
probability that this "true" value is in the 90% interval, given our model,
prior information, and the data.

The 90% interval contains the middle 90% of the parameter values.

There is a 5% chance, says the model, the height parameter is below the 5% 
quantile.

```{r}
posterior_interval(stan_model)
```


## ROPE

ROPE: Region of practical equivalence.

Let's say that the height effect is practically equivalent to 0 when it fall
between -0.1 and 0.1. How many values are in this ROPE?

```{r}
height_effects <- as.data.frame(stan_model)[["heightC"]]
mean(-.1 <= height_effects & height_effects <= .1)
```



## Plotting the parameters

Basic `plot()` method shows 80% and 95% intervals.

```{r basic-parameter-plot, fig.cap = "Basic plot() called on model. Shows 80%, 95% intervals for each parameter."}
plot(stan_model)
```




## Plotting some of the parameters

```{r subset-of-parameters, fig.cap = "Basic plot() called on model. Shows 80%, 95% intervals. Limited to just parameters matching 'height'."}
# Match a subset of the parameters
plot(stan_model, regex_pars = "height") 
```

<!-- ## What is the predicted value for each sex? -->


<!-- ```{r} -->
<!-- new_data <- davis -->
<!-- ``` -->




[osf2015]: http://science.sciencemag.org/content/349/6251/aac4716

[arm]: https://cran.rstudio.com/web/packages/arm/

