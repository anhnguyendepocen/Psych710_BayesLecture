---
title: "Bayesian regression"
author: "Tristan Mahr"
date: "April 13, 2017"
output:
  beamer_presentation:
    includes:
      in_header: header.txt
    latex_engine: pdflatex
    theme: Metropolis
    toc: no
  github_document: default
  ioslides_presentation: default
subtitle: subtitle
---

```{r setup, include = FALSE}
options(width = 60)
knitr::opts_knit$set(width = 60)

knitr::opts_chunk$set(
  echo = FALSE,
  comment = "#>",
  collapse = TRUE)

library(dplyr)
library(ggplot2)
library(extrafont)
library(hrbrthemes)

extrafont::loadfonts()

my_theme <- theme_ipsum(base_size = 24, axis_title_size = 20)

theme_set(my_theme)
```

## Overview

* A little about me and how I got into Bayes
* Bayes theorem as a way to unify all of statistics
* A simple regression model
* Things to do with posteriors
* Where to learn more

# Background

## About me 

* I am dissertator in Communication Sciences and Disorders
* I study word recognition in preschoolers
* For stats, I do multilevel logistic regression models
* R enthusiast

## I was once in your shoes

I learned stats and R in this course with Markus Brauer and John Curtin.

I _still_ refer to the slides from this course on contrast codes.

But now I'm a "Bayesian"


## August 2015: The "Crisis" in Psychology

[Open Science Collaboration (2015)][osf2015] tries to replicate 100 studies
published in 3 psychology different journals in 2008.

- Boil a study down to 1 test statistic and 1 effect size.
- Replicate the study.
- Compare replication's test statistical and effect size against original.

----

![Scatter plot of original vs replicated effect sizes](./assets/reproducibility.PNG)

----

- Approximately 36% of the studies are replicated (same test statistic.
- On average, effect sizes in replications are half that of the original
  studies.



## Reactions

![I don't know how to turn off the figure labeling feature](./assets/this-is-fine.jpeg)


## Reactions

> * We're doomed.
> * Most findings are probably false.
> * No, this is business as usual.
> * Any credible discipline has to do this kind of house-cleaning from time to time.




## Lots of hand wringing and soul searching

Some reactionary:

- Replication creates an [industry for incompetent hacks][replication-hacks].
- Here come the [methodological terrorists][method-terrorists]!

Some constructive:

- [Everything is f'ed][everything-fucked] -- so what else is new?
- Increased rigor and openness are [a good thing][repeat-after-me].

[replication-hacks]: http://www.sciencedirect.com/science/article/pii/S002210311600007X
[method-terrorists]: http://andrewgelman.com/2016/09/21/what-has-happened-down-here-is-the-winds-have-changed/
[everything-fucked]: https://hardsci.wordpress.com/2016/08/11/everything-is-fucked-the-syllabus/
[repeat-after-me]: https://thenib.com/repeat-after-me



## Crisis made me think more about questionable practices

All those unintentional acts and rituals to appease the Statistical Significance
gods.

HARKing 
~ Hypothesizing after results are known.
~ Telling a story to fit the data.

Garden of forking data
~ Conducting countless sub-tests and sub-analyses on the data.

_p_-hacking
~ Doing these tests in order to find a significant effect.

Selective reporting
~ Reporting only the tests that yielded a significant result.

----

The usual way of doing things is _insecure_.

----


The usual way of doing things is _insecure_.

- Perfectly fine if you know what you're doing.
- But vulnerable to exploitation.


----

## My response to the crisis


I want to avoid these traps.

I want to level up my stats and explore new techniques.

- Maybe more robust estimation techniques?
- Maybe machine learning techniques to complement conventional analyses?

I want something less finicky than statistical significance.

- _p_-values don't mean what many people think they mean.
- Neither do confidence intervals.
- Statistical significance is not related to practical significance.



## December 2015

![Cover of Data Analysis USing Regression and Multilevel/Hierarchical Models](./assets/arm.jpeg)

---

I started reading the Gelman and Hill book.

- This is the book for [the `arm` package][arm].
- Still the best treatment of multilevel models in R despite being 10 years old.

It emphasizes estimation, uncertainty and simulation.

Midway through, the book pivots to Bayesian estimation.

[arm]: https://cran.rstudio.com/web/packages/arm/


## January 2016

I'm down a rabbit hole, writing Stan (Bayesian) models to fit the models from the ARM
book, and there is an influx of Bayesian tools for R.

- [Statistical Rethinking][rethinking], a book that reteaches regression from a
  Bayesian perspective with R and Stan, is released.
- New version of [brms][brms] is released. This package converts R model code
  into Stan programs.
- [RStanARM][rstanarm] is released.
- A blog post circulates: ["R Users Will Now Inevitably Become Bayesians"]
  [we-r-bayesians].

I eat all this up. I become a convert.

[rethinking]: http://xcelab.net/rm/statistical-rethinking/
[brms]: https://cran.r-project.org/web/packages/brms/index.html
[rstanarm]: https://cran.r-project.org/web/packages/rstanarm/index.html
[we-r-bayesians]: https://thinkinator.com/2016/01/12/r-users-will-now-inevitably-become-bayesians/



## Long story short

The replication crisis sparked my curiosity, and a wave of new tools and
resources made it really easy to get started with Bayesian stats.

My goal with this approach has been to make better, more honest scientific
summaries of observed data.






# Building mathematical intuitions


## Caveat

* These slides and these examples are meant to illustrate the pieces of Bayes
  theorem.
* This is not a rigorous mathematical description of Bayesian probability
  or regression.


## Conditional probability review

$$ p(A \mid B) : \text{probability of A given B}$$

. . .

Suppose that 95% of emails with the phrase "investment opportunity" are spam.

. . .

$$ p(\text{spam email} \mid \text{"investment opportunity"})  = .95 $$


## Conditional probability review

What would this probability express?

$$ p(\text{"investment opportunity"} \mid \text{spam email})$$

. . .

That ordering matters. $p(A \mid B)$ is not the same as $p(B \mid A)$.




## Bayes theorem

A theorem about conditional probability.

$$ p(B \mid A) = \frac{ p(A \mid B) * p(B)}{p(A)} $$

----

I can never remember this equation with letters. Here's how I prefer to write it.

$$ p(\text{hypothesis} \mid \text{data}) = \frac{ p(\text{data} \mid \text{hypothesis}) * p(\text{hypothesis})}{p(\text{data})} $$

---

$$ p(\text{hypothesis} \mid \text{data}) = \frac{ p(\text{data} \mid \text{hypothesis}) * p(\text{hypothesis})}{p(\text{data})} $$

The "hypothesis" is typically something unobserved or unknown. It's what you 
want to learn about using the data. 


For regression models, the "hypothesis" is a parameter (intercept, slopes or 
error terms).

Bayes theorem tells you the probability of the hypothesis given the data.



## General structure

How plausible is some hypothesis given the data?

$$ p(\text{hypothesis} \mid \text{data}) = \frac{ p(\text{data} \mid \text{hypothesis}) * p(\text{hypothesis})}{p(\text{data})} $$

. . .

Pieces of the equation:

$$ \text{posterior} = \frac{ \text{likelihood} * \text{prior}}{\text{average\ likelihood}} $$


## Classifying emails

I got an email with the word "cialis" in it. Is it spam?

. . .

- What I want to know is spam-ness (versus ham-ness). 
- What I have is an email with the word "cialis".

. . .

$$ P(\text{spam} \mid \text{"cialis"}) = \frac{ P(\text{"cialis"} \mid \text{spam}) * P(\text{spam})}{P(\text{"cialis"})} $$


## Email example

The two unconditional probabilities are base rates that need to be accounted
for.

. . .

The prior is the frequency of spam in general. The average likelihood is the
frequency of the word "cialis" in emails.

$$ P(\text{spam} \mid \text{"cialis"}) = \frac{\text{"cialis"\ freq.\ in\ spam} * \text{spam\ rate}}{\text{"cialis"\ freq.}} $$


## "Bayesianism" 

Some people would argue that using Bayes theorem is not "Bayesian". After
all, in this example, we're just counting the frequency of events. 

It's kind of weird, but it is also true. 

Simple event-counting is not what people usually mean by the word "Bayesian". 

--------


## The "Bayesianism" form of Bayes' theorem

$$ \text{updated information} = \frac{\text{likelihood of data} * \text{prior information}}{\text{average\ likelihood of data}} $$

Bayes' theorem provides a systematic way to update our knowledge as we encounter new data.

. . .

$$ \text{updated beliefs} \propto \text{likelihood of data} * \text{prior beliefs} $$

- Update your beliefs in proportion to how well the data fits those beliefs.
- Your beliefs have probabilities. You can quantify your uncertainty about what you know.


# Making Bayes theorem work with regression


## ok 

You probably write regression models as a one-liner like:


$$ \underbrace{y_i}_{observation} = 
   \underbrace{\alpha + \beta _{1} x_{1i}}_{\text{predicted mean given \it{x}}} + 
   \underbrace{\epsilon _i}_{\text{random error}} $$ 

. . . 

Data generating model: Observation $y_i$ is a draw from a normal distribution
centered around a mean.

We estimate the mean with a constant "intercept" term $\alpha$ plus a linear
combination of predictor variables (just $x_1$ for now).

----

Let's re-write the model to make the normal-distribution part clear. No more 
one-liner. (The $\sim$ means "sampled from".)

\begin{align*}
   y_i &\sim \mathrm{Normal}(\mathrm{mean} = \mu_i, \mathrm{SD} = \sigma)
   \\
  \mu_i &= \alpha + \beta_1*x_{1i}
\end{align*}

. . . 

Observation $y_i$ is a draw from a normal distribution
centered around a mean $\mu_i$ with a standard deviation of $\sigma$.

The mean is a constant term $\alpha$ plus a linear combination of predictor
variables (just $x_1$ for now).

----

(These equations describe the same models. It's just a different kind of
notation.)

----


```{r staggered-bell-curves, echo = FALSE, fig.cap = "Scatter plot with bell curves overlaid to how the shape of the density remains the same. It's just the center that moves with x. ", message = FALSE, warning = FALSE}
# options(hrbrthemes.loadfonts = TRUE)

# Some toy data
davis <- car::Davis %>%
  filter(100 < height)

# Fit a model and estimate mean at five points
m <- lm(weight ~ height, davis)
newdata <- data_frame(height = c(15:19 * 10))
newdata$fit <- predict(m, newdata)

# get density of random normal values
get_density_df <- function(mean, sd, steps) {
  ends <- qnorm(c(.001, .999), mean, sd)
  steps <- seq(ends[1], ends[2], length.out = steps)

  df <- data_frame(
    value = steps,
    density = dnorm(steps, mean, sd))
  df
}

# Get a distribution at each mean
simulated <- newdata %>%
  group_by(height) %>%
  do(get_density_df(.$fit, sigma(m), 10000)) %>%
  ungroup


ggplot(simulated) +
  # Plot at each mean, adding some scaled value of density to the mean.
  aes(x = height - (100 * density), y = value, group = height) +
  geom_polygon(fill = "grey50") +
  # raw data
  geom_point(aes(height, weight), data = davis) +
  labs(x = "height (cm)", y = "weight (kg)") + 
  my_theme
```


# What is likelihood?


----

_Sidenote: This is nifty. A lot of my stats training made more sense
once I had a broader understanding of likelihood._

----

## I have a model

What is a statistical model?

. . .

It's a description of how the data could have been generated.




## Likelihood measures fit

How likely are the data in a given model? 

. . . 

I never see it explained this way, but I think of likelihood as "fit".

How the well data fits in a given model.





## {.fragile}

We found some IQ scores in an old, questionable dataset.

```{r some-code, echo = TRUE}
library(dplyr)
iqs <- car::Burt$IQbio
iqs
```

IQs are designed to have a normal distribution with a population mean of 100
and an SD of 15.

How well do these data *fit* in that kind of bell curve?

----

## Density as height on a bell curve

```{r, echo = FALSE}
iq_df <- function(mean, sd = 15, xs = iqs) {
  data_frame(
    iq = seq(min(xs), max(xs), length.out = 100),
    density = dnorm(iq, mean, 15),
    mean = mean,
    sd = sd)
}
```

```{r iq-density-bell-curve, echo = FALSE, fig.cap = "A hypothetical bell curve with a mean of 100 and SD of 15."}
p <- ggplot(iq_df(100, 15)) +
  aes(iq, density) +
  geom_line(size = 1)
p
```

## Density as height on a bell curve

```{r iq-density-bell-curve-2, echo = FALSE, fig.cap = "Likelihood of an IQ of 90"}
one_iq <- data_frame(iq = 90, density = dnorm(90, 100, 15), yend = 0, xend = 90)

p + 
  geom_segment(aes(xend = xend, yend = yend), data = one_iq, size = 1) +
  geom_point(data = one_iq, size = 3)
```



---







* Height of each point on curve is density around that point.
* Higher density regions are more likely.
* Data farther from peak density is less likely.

----



```{r iq-density-bell-curve-3, echo = FALSE, fig.cap = "Density of IQ scores drawn a bell curve with mean 100."}
p <- ggplot(iq_df(100, 15)) +
  aes(iq, density) +
  geom_line(size = 1) +
  geom_point(aes(x = iqs, y = dnorm(iqs, 100, 15)), data = data_frame(iqs), size = 3) +
  geom_segment(aes(x = iqs, xend = iqs, y = 0, yend = dnorm(iqs, 100, 15)),
               data = data_frame(iqs), size = 1) 
p
```



----

```{r iq-density-bell-curve-4, echo = FALSE, fig.cap = "Density of IQ scores drawn a bell curve with mean 130. The fit is terrible."}
bad_mean <- 130

p <- ggplot(iq_df(bad_mean, 15)) +
  aes(iq, density) +
  geom_line(size = 1) +
  geom_point(aes(x = iqs, y = dnorm(iqs, bad_mean, 15)), data = data_frame(iqs), size = 3) +
  geom_segment(aes(x = iqs, xend = iqs, y = 0, yend = dnorm(iqs, bad_mean, 15)),
               data = data_frame(iqs), size = 1) 
p
```



[osf2015]: http://science.sciencemag.org/content/349/6251/aac4716


